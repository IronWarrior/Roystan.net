---
layout: article.njk
title: Unity Outline Shader Tutorial - Roystan
name: Outline Shader
date: 2019-02-09
thumbnail: /media/tutorials/outline-thumb.png
tool: Unity engine
toolSuffix: 2018.3
duration: 50
ogTitle: Unity Outline Shader Tutorial at Roystan
description: Learn to write an outline edge detection shader for Unity engine, integrated with the post-processing stack. This effect is especially popular as a compliment to toon shading, or in CAD and architectural rendering.
---
			<img src="/media/tutorials/outline-demo.png" alt="Primitive 3D objects rendered with white outlines in Unity engine." class="figure-header-cc header-image"></img>
			<div class="tutorial-info">
				<p><span class="drop">You will learn</span> to write a screen space shader to draw outlines around objects. This shader will be integrated with Unity's post-processing stack.</p>
			</div>
			<p><strong>Outline</strong>, or <strong>edge detection</strong> effects are most commonly associated and paired with toon style shading. However, outline shaders have a wide variety of uses, from highlighting important objects on screen to increasing visual clarity in CAD rendering.</p>
			<p>This tutorial will describe step-by-step how to write an outline shader in Unity. The shader will be written as a custom effect for Unity's <a href="https://github.com/Unity-Technologies/PostProcessing/wiki" target="_blank">post-processing stack</a>, but the code can also be used in a regular image effect. Concepts introduced here can be found in the <strong>Recolor</strong> effect from the <a href="https://github.com/keijiro/Kino" target="_blank">Kino repository by keijiro</a>, a Unity project that contains a variety of custom effects for the post-processing stack.</p>
			<div class="project-info clearfix">
				<div class="project-info-note"><strong>The completed project</strong> is provided at the end of the article. Note that it also contains a large amount of comments in the created shader and C# files to aid understanding.</div>
			</div>
			<h3>Prerequisites</h3>
			<p>To complete this tutorial, you will need a working knowledge of Unity engine, and an intermediate knowledge of shaders.</p>
			<a class="button" href="https://github.com/IronWarrior/UnityOutlineShader/archive/skeleton.zip">
				<span>Download starter project</span>
				<span class="button-type">.zip</span>
			</a>
			<div class="patreon-info">
				<p>These tutorials are made possible, and kept <strong>free</strong> and <strong>open source</strong>, by your support. If you enjoy them, please consider becoming my <a href="https://www.patreon.com/bePatron?u=10821388" target="_blank">patron through Patreon</a>.</p> 
				<a href="https://www.patreon.com/bePatron?u=10821388" data-patreon-widget-type="become-patron-button">Become a Patron!</a><script async src="https://c6.patreon.com/becomePatronButton.bundle.js"></script>
			</div>
			<h2>Getting started</h2>
			<p>Download the starter project provided above, open it in the Unity editor and open the <code>Main</code> scene. In this scene are several objects, each with a different shape and silhouette. This gives us a variety of surface types to test our outline shader on.</p>
			<div class="figure">
				<img src="/media/tutorials/outline-start.png"></img>
			</div>
			<p>If you select the <code>Main Camera</code>, you will note that already attached to it are the <code>Post Process Layer</code> and <code>Post Process Volume</code> components. These components allow us to make use of the post-processing stack. Assigned to the <code>Profile</code> field of the volume is a post-process profile, <strong>OutlinePostProfile</strong>. This will contain data that we will use to configure our outline effect.</p>
			<p>Note that by default <code>Anti-aliasing</code> in the layer is set to <em>No Anti-aliasing</em>. It can useful to keep anti-aliasing disabled when developing screen space shaders, as it allows you to see the end product of the shader without any further modification applied. For the "finished" screenshots in this tutorial, and for best results, anti-aliasing is set to <strong>Subpixel Morphological Anti-aliasing (SMAA)</strong> at <strong>High</strong> quality.</p>
			<h2>Writing custom post-process effects</h2>
			<p>Open the <code>Outline</code> shader in your preferred code editor. Shaders written for Unity's post-processing stack have a few differences compared to standard image effects. Although the shader code itself is the same, it is encapsulated in <code>HLSLPROGRAM</code> blocks, instead of <code>CGPROGRAM</code>. As well, some functionality, such as texture sampling, is now handled by macros.</p>
			<p>Currently, the <code>Outline</code> file contains a simple fragment shader (named <code>Frag</code>) that samples the image on-screen and returns it without modification. There is also a function named <code>alphaBlend</code> defined; we will use it later for blending our outlines with the on-screen image.</p>
			<aside>
				<div class="aside-button">
					<h4>Why use the post-processing stack at all? Why not just write a standard image effect?</h4>
				</div>
				<div class="aside-content">
					<p>By integrating our shader with the post-processing stack, we gain access to powerful built-in anti-aliasing solutions. As well, our effect will be automatically compatible with other post-process effects in the stack, like the <strong>Bloom</strong> shown below.</p>
					<img src="/media/tutorials/outline-bloom.png"></img>
				</div>
			</aside>
			<h2>1. Drawing outlines with depth</h2>
			<p>To generate outlines, we will sample adjacent pixels and compare their values. If the values are very different, we will draw an edge. <a href="https://en.wikipedia.org/wiki/Sobel_operator" target="_blank">Some edge detection algorithms</a> work with grayscale images; because we are operating on computer rendered images and not photographs, we have better alternatives in the <strong>depth</strong> and <strong>normals</strong> buffers. We will start by using the depth buffer.</p>
			<p>We will sample pixels from the depth buffer in a <em>X</em> shape, roughly centred around the current pixel being rendered. Add the following code to the top of the fragment shader.</p>
<pre><code class="language-glsl">float halfScaleFloor = floor(_Scale * 0.5);
float halfScaleCeil = ceil(_Scale * 0.5);

float2 bottomLeftUV = i.texcoord - float2(_MainTex_TexelSize.x, _MainTex_TexelSize.y) * halfScaleFloor;
float2 topRightUV = i.texcoord + float2(_MainTex_TexelSize.x, _MainTex_TexelSize.y) * halfScaleCeil;  
float2 bottomRightUV = i.texcoord + float2(_MainTex_TexelSize.x * halfScaleCeil, -_MainTex_TexelSize.y * halfScaleFloor);
float2 topLeftUV = i.texcoord + float2(-_MainTex_TexelSize.x * halfScaleFloor, _MainTex_TexelSize.y * halfScaleCeil);</code></pre>
			<p>We first calculate two values, <code>halfScaleFloor</code> and <code>halfScaleCeil</code>. These two values will alternatively increment by one as <code>_Scale</code> increases. By scaling our UVs this way, we are able to increment our edge width exactly one pixel at a time—achieving a maximum possible granularity—while still keeping the coordinates centred around <code>i.texcoord</code>.</p>
			<div class="figure">
				<img src="/media/tutorials/pixel-grid.gif" class="medium"></img>
			</div>
			<aside>
				<div class="aside-button">
					<h4>Why is it important to increment one pixel at a time? Why not half a pixel, or even less?</h4>
				</div>
				<div class="aside-content">
					<p>Both the normals and depth buffers are by default sampled using <strong>point filtering</strong>. This means that you cannot sample a point "in between" pixels and retrieve a blended result. For this reason, we make sure our UVs are incremented one pixel at a time to ensure we are always correctly sampling the buffers.</p>
					<img src="/media/tutorials/point-filter-comparison.png"></img>
					<p>The above demonstrates a comparison between <strong>point filtering</strong> (left) and <strong>bilinear filtering</strong> (right). Note that it is possible to sample textures with different filtering types inside a shader using <a href="https://docs.unity3d.com/Manual/SL-SamplerStates.html" target="_blank">inline sampler states</a>, but for our purposes we will continue to use point filtering.</p>
				</div>
			</aside>
			<p>Next, <code>_Scale</code> will need to be added as a configurable property. Properties are created a bit differently with the post-processing stack. We will first define it as <code class="variable">float</code> is our shader program, as usual. Add the following code below the <code>float4 _MainTex_TexelSize</code> line.</p>
<pre><code class="language-glsl">float _Scale;</code></pre>
			<p>Next, open the <code>PostProcessOutline.cs</code> file. This file contains classes that manage rendering our custom effect and exposing any configurable values to the editor. We will expose <code>_Scale</code> as a parameter, and pass it into our shader.</p>
<pre><code class="language-csharp">// Add to the PostProcessOutline class.
public IntParameter scale = new IntParameter { value = 1 };

…

// Add to the Render method in the PostProcessOutlineRenderer class, just below var sheet declaration.
sheet.properties.SetFloat("_Scale", settings.scale);</code></pre>
			<p>If you select the <code>OutlinePostProfile</code> asset now, you will see that <code>Scale</code> has been exposed to the inspector. We'll leave it at 1 for now.</p>
			<p>We are now ready to sample the depth texture using our four UV coordinates.</p>
<pre><code class="language-glsl">// Add to the fragment shader, just below float2 topLeftUV.
float depth0 = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, sampler_CameraDepthTexture, bottomLeftUV).r;
float depth1 = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, sampler_CameraDepthTexture, topRightUV).r;
float depth2 = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, sampler_CameraDepthTexture, bottomRightUV).r;
float depth3 = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, sampler_CameraDepthTexture, topLeftUV).r;

return depth0;</code></pre>
			<div class="figure">
				<img src="/media/tutorials/outline-depth.png"></img>
				<div class="figure-info">Depth buffer outputted to the screen. To increase visual clarity for this image, the <strong>Near</strong> plane of the camera has been set to 2 (the default is 0.3).</div>
			</div>
			<aside>
				<div class="aside-button">
					<h4>We now have multiple return calls in our fragment shader, is this correct?</h4>
				</div>
				<div class="aside-content">
					<p>Yes. The fragment shader will return the value provided at the first return call and cease executing; this is useful for debugging your shader's progress as you go. For production code it is not recommended to have multiple return calls, as it typically looks like an error on the programmer's part.</p>
				</div>
			</aside>
			<p>As previously stated, effects integrated with the post-processing stack use a variety of macros to ensure multi-platform compatibility. Here we use <code>SAMPLE_DEPTH_TEXTURE</code> on the camera's depth texture. We only take the <code>r</code> channel, as depth is a <strong>scalar value</strong>, in the 0...1 range. Note that depth is <em>non-linear</em>; as distance from the camera increases, smaller depth values represent greater distances.</p>
			<p>With our values sampled, we can now compare the depth of pixels across from each other through subtraction. Note that existing code that is <strong>modified</strong> will be <span class="modified-inline">highlighted in yellow</span>. New code is <em>not</em> highlighted.</p></p>
<pre><code class="language-glsl">// Add above the return depth0 line.
float depthFiniteDifference0 = depth1 - depth0;
float depthFiniteDifference1 = depth3 - depth2;

…

// Replace the existing return depth0 line.
</code><span class="modified"><code>return abs(depthFiniteDifference0) * 100;</code></span></pre>
			<p>As the difference can be positive or negative, we take the absolute value of it before returning the result. Since the difference between nearby depth values can be very small (and therefore difficult to see on screen), we multiply the difference by 100 to make it easier to see.</p>
			<p><code>depthFiniteDifference0</code> is half of the detected edges, while <code>depthFiniteDifference1</code> is the other half. You can switch the return value between the two to see the difference.</p>
			<div class="figure">
				<img src="/media/tutorials/outline-finite-diff.gif"></img>
				<div class="figure-info">Alternating between outputting <strong>depthFiniteDifference0</strong> and <strong>depthFiniteDifference1</strong>.</div>
			</div>
			<p>We now have two scalar values representing the intensity of detected outlines in our image; they will now need to be combined into one. There are several trivial ways to do this, from simply adding the two values together, to plugging them into the <code>max</code> function. We will compute the <em>sum of squares</em> of the two values; this is part of an edge detection operator called <a href="https://en.wikipedia.org/wiki/Roberts_cross">the Roberts cross</a>.</p>
			<p>The <strong>Roberts cross</strong> involves taking the difference of diagonally adjacent pixels (we have already done this), and computing the sum of squares of the two values. To do this, we will <strong>square</strong> both our values, <strong>add</strong> them together, and then <strong>square root</strong> the result.</p>
<pre><code class="language-glsl">// Add below the lines computing the finite differences.			
float edgeDepth = sqrt(pow(depthFiniteDifference0, 2) + pow(depthFiniteDifference1, 2)) * 100;

// Replace the abs(depthFiniteDifference0) * 100 line.
</code><span class="modified"><code>return edgeDepth;</code></span></pre>
			<div class="figure">
				<img src="/media/tutorials/outline-roberts.png"></img>
			</div>
			<p>You will notice that while the edges come in very clear, there's a lot of dark grey areas along the surfaces of our objects. We will elimate these by <em>thresholding</em> <code>edgeDepth</code>. Add the following code to the shader file...</p>
<pre><code class="language-glsl">// Add below the line declaring edgeDepth.
edgeDepth = edgeDepth > _DepthThreshold ? 1 : 0;	

…

// Add as a new variable.
float _DepthThreshold;</code></pre>
			<p>...and add the code below to <code>PostProcessOutline.cs</code>.</p>
<pre><code class="language-csharp">// Add to the PostProcessOutline class.
public FloatParameter depthThreshold = new FloatParameter { value = 0.2f };

…

// Add below the line setting _Scale.
sheet.properties.SetFloat("_DepthThreshold", settings.depthThreshold);</code></pre>
			<div class="figure">
				<img src="/media/tutorials/outline-threshold.png"></img>
			</div>
			<p>While this has eliminated the dark greys, it has created a few issues. The top of one of the foreground cubes is filled in white, instead of just the edges. As well, the cubes in the background have no edges drawn between their silhouettes. We'll fix the problem with the background cubes for now, and will resolve the foreground one later.</p>
			<p>Edges are drawn between areas where the <code>edgeDepth</code> is <em>greater</em> than <code>_DepthThreshold</code>, a constant. It was stated earlier that the depth buffer is <strong>non-linear</strong>, which has implications for our thresholding. Two cubes a meter apart that are near the camera will have a much larger <code>edgeDepth</code> between them than two cubes that are very far from the camera.</p>
			<p>To accommodate this, we will modulate <code>_DepthThreshold</code> based on the existing depth of our surfaces.</p>
<pre><code class="language-glsl">// Add below the line declaring edgeDepth.
float depthThreshold = _DepthThreshold * depth0;
edgeDepth = edgeDepth > </code><span class="modified"><code>depthThreshold</code></span><code> ? 1 : 0;</code></pre>
			<p><code>_DepthThreshold</code> is now too small for our new equation; set its value to <strong>1.5</strong> for better results.</p>
			<div class="figure">
				<img src="/media/tutorials/outline-depth-threshold.png"></img>
			</div>
			<p>This has resolved the issue with the background cubes, but also has created more surface artifacts. As well, many edges (such as those along the staircase) were not detected, as the <code>edgeDepth</code> values between steps was too small. To correctly draw outlines on these surfaces, we will make use of the <strong>normals buffer</strong>.</p>
			<h2>2. Drawing outlines with normals</h2>
			<p>We will now repeat the previous process, except this time using the normals buffer instead of depth. At the end, we will combine the results of the two for maximum edge coverage. Add the following to the fragment shader, below the code sampling the depth buffer.</p>
<pre><code class="language-glsl">float3 normal0 = SAMPLE_TEXTURE2D(_CameraNormalsTexture, sampler_CameraNormalsTexture, bottomLeftUV).rgb;
float3 normal1 = SAMPLE_TEXTURE2D(_CameraNormalsTexture, sampler_CameraNormalsTexture, topRightUV).rgb;
float3 normal2 = SAMPLE_TEXTURE2D(_CameraNormalsTexture, sampler_CameraNormalsTexture, bottomRightUV).rgb;
float3 normal3 = SAMPLE_TEXTURE2D(_CameraNormalsTexture, sampler_CameraNormalsTexture, topLeftUV).rgb;</code></pre>
			<p>Attached to the camera is a script called <code>RenderReplacementShaderToTexture</code>, setup to generate a camera to render the <strong>view-space normals</strong> of the scene into <code>_CameraNormalsTexture</code>. We will once again take the difference between these samples to detect outlines.</p>
			<div class="figure">
				<img src="/media/tutorials/outline-view-space-normals.png"></img>
				<div class="figure-info">View-space normals of the scene. These are the normals of the objects relative to the camera.</div>
			</div>
			<p>Note that going forward, you will <strong>need to run the scene</strong> to get the correct results, as the camera that renders out the normals is generated at runtime.</p>
<pre><code class="language-glsl">// Add below the code sampling the normals.				
float3 normalFiniteDifference0 = normal1 - normal0;
float3 normalFiniteDifference1 = normal3 - normal2;

float edgeNormal = sqrt(dot(normalFiniteDifference0, normalFiniteDifference0) + dot(normalFiniteDifference1, normalFiniteDifference1));
edgeNormal = edgeNormal > _NormalThreshold ? 1 : 0;

return edgeNormal;

…

// Add as a new variable.
float _NormalThreshold;</code></pre>
			<p>The above process is very similar to what we did with depth, with some differences in how we compute the edge. As our <code>normalFiniteDifference</code> values are <strong>vectors</strong>, and not <strong>scalars</strong>, we need to transform them from a 3-dimensional value to a single dimensional value before computing the edge intensity. The <a href="https://en.wikipedia.org/wiki/Dot_product" target="_blank">dot product</a> is ideal for this; not only does it return a scalar, but by performing the dot product for each <code>normalFiniteDifference</code> on <em>itself</em>, we are also squaring the value.</p>
			<p>Because we added <code>_NormalThreshold</code> as a new variable, we will need to expose it in <code>PostProcessOutline.cs</code>.</p>
<pre><code class="language-csharp">// Add to the PostProcessOutline class.
[Range(0, 1)]
public FloatParameter normalThreshold = new FloatParameter { value = 0.4f };

…

// Add to the Render method in the PostProcessOutlineRenderer class.
sheet.properties.SetFloat("_NormalThreshold", settings.normalThreshold);</code></pre>
			<div class="figure">
				<img src="/media/tutorials/outline-normals.png"></img>
			</div>
			<p>Some new edges, notably those along the staircase's steps, are now visible, while some edges that were previously visible no longer are. To resolve this, we will combine the results of the depth and normal edge detection operations using the <code>max</code> function.</p>
<pre><code class="language-glsl">// Remove the return calls we were using for debugging.
</code><span class="removed"><code>return edgeNormal;</code></span><code class="language-glsl">

…

</code><span class="removed"><code>return edgeDepth;</code></span><code class="language-glsl">

…

// Add at the bottom, just above the line declaring float4 color.
float edge = max(edgeDepth, edgeNormal);
return edge;</code></pre>
			<div class="figure">
				<img src="/media/tutorials/outline-combined.png"></img>
			</div>
			<h2>3. Resolving surface artifacts</h2>
			<p>There are a number of artifacts visible on some of the surfaces that lie at <strong>sharp angles</strong> with respect to the camera. The greater the slope of a surface, the greater the difference between the depth of adjacent pixels. This large depth delta along these surfaces is causing our algorithm to detect "edges" on them.</p>
			<div class="figure">
				<img src="/media/tutorials/cube-outline-spin.gif" class="medium"></img>
			</div>
			<p>One option to remove these artifacts is to simply increase <code>_DepthThreshold</code>. While setting it to a value of 6 removes the artifacts entirely, it also is too large a threshold for some outlines that <em>should</em> be detected, like the teapot's rim or some staircase steps.</p>
			<div class="figure">
				<img src="/media/tutorials/outline-depth-comparison.gif"></img>
			</div>
			<p>Instead, we will modulate <code>depthThreshold</code> by the surface's normal. Surfaces that are at a greater angle from the camera with have a <strong>larger</strong> threshold, while surface that are flatter, or more <em>planar</em> to the camera will have a <strong>lower</strong> threshold.</p>
			<p>To implement this we will need the normal of each surface, and the direction from the camera to the surface (the <em>view direction</em>). We already have the normal, but we <strong>don't</strong> have access to the view direction.</p>
			<h3>3.1 Calculating view direction</h3>
			<p>The normals we sampled from <code>_CameraNormalsTexture</code> are in <strong>view space</strong>; since these are what we want to compare against, we will need the camera's view direction to also be in view space. As we are working with a screen space shader, the view direction in <em>clip space</em> can be easily calculated from the vertex position. To convert this to view space, we'll need access to the camera's <strong>clip to view</strong>, or <strong>inverse projection</strong> matrix.</p>
			<p>This matrix is not available by default to screen space shaders; we will calculate it in our C# script and pass it into our shader from there. Add the following just above the line calling <code>BlitFullscreenTriangle</code>...</p>
<pre><code class="language-csharp">Matrix4x4 clipToView = GL.GetGPUProjectionMatrix(context.camera.projectionMatrix, true).inverse;
sheet.properties.SetMatrix("_ClipToView", clipToView);</code></pre>
			<p>...and add the code below as variables to our shader.</p>
<pre><code class="language-glsl">float4x4 _ClipToView;</code></pre>
			<p>The view to clip (called the <em>projection</em> matrix here) is exposed in the <a href="https://docs.unity3d.com/ScriptReference/Camera.html" target="_blank">Camera class</a>. Note that we take the <strong>inverse</strong> of the matrix, as we are transforming our direction from clip to view space, not the other way around. Due to platform differences, it is important to plug the projection matrix into the <code>GetGPUProjectionMatrix</code> function. This ensures that the resulting matrix is correctly configured for our shader.</p>
			<p>We can now calculate the view direction in view space. This operation will need to be done in the <strong>vertex shader</strong>. Up until now, we have been using the built-in <code>VertDefault</code> as our vertex shader. The source code for this shader is available in <code>StdLib.hlsl</code>, which we have included in our file. We'll copy this shader over, and then make some modifications.</p>
<pre><code class="language-glsl">// Replace VertDefault with our new shader.
#pragma vertex </code><span class="modified"><code>Vert</code></span><code class="language-glsl">

…

// Add below the alphaBlend function.
struct Varyings
{
	float4 vertex : SV_POSITION;
	float2 texcoord : TEXCOORD0;
	float2 texcoordStereo : TEXCOORD1;
#if STEREO_INSTANCING_ENABLED
	uint stereoTargetEyeIndex : SV_RenderTargetArrayIndex;
#endif
};

Varyings Vert(AttributesDefault v)
{
	Varyings o;
	o.vertex = float4(v.vertex.xy, 0.0, 1.0);
	o.texcoord = TransformTriangleVertexToUV(v.vertex.xy);

#if UNITY_UV_STARTS_AT_TOP
	o.texcoord = o.texcoord * float2(1.0, -1.0) + float2(0.0, 1.0);
#endif

	o.texcoordStereo = TransformStereoScreenSpaceTex(o.texcoord, 1.0);

	return o;
}

…

// Update the fragment shader's declaration to take in our new Varyings struct, instead of VaryingsDefault.
float4 Frag(</code><span class="modified"><code>Varyings</code></span><code class="language-glsl"> i) : SV_Target</code></pre>
			<p>In addition to copying over the vertex shader, we have also copied the default <code class="variable">struct</code> that is passed from the vertex shader, <code>Varyings</code>. This will allow us to pass the view direction to our fragment shader.</p>
			<p>The clip space position (which ranges from <code>-1, -1</code> at the  top left of the screen to <code>1, 1</code> at the bottom right) can be interpreted as a the camera's view direction to each pixel, in clip space. This position is already calculated and stored in <code>o.vertex</code>. We will multiply this value by our matrix to transform the direction to view space.</p>
			<div class="figure">
				<img src="/media/tutorials/clip-space.png"></img>
				<div class="figure-info">Clip space positions of the vertices, as stored in <strong>o.vertex</strong> in the vertex shader. The <strong>x</strong> coordinates are stored in the <strong>red</strong> channel, while the <strong>y</strong> coordinates are in the <strong>green</strong> channel.</div>
			</div>
<pre><code class="language-glsl">// Add to the vertex shader, below the line assigning o.vertex.
o.viewSpaceDir = mul(_ClipToView, o.vertex).xyz;

…

// Add to the Varyings struct.
float3 viewSpaceDir : TEXCOORD2;</code></pre>
			<p>You can debug this value out by adding the following to the top of our fragment shader.</p>
<pre><code class="language-glsl">return float4(i.viewSpaceDir, 1);</code></pre>
			<p>Make sure to remove this line of code after you have observed its results, as we will not use it any further.</p>
			<h3>3.2 Thresholding with view direction</h3>
			<p>We are going to modulate <code>depthThreshold</code> based on the difference between the camera's viewing normal and the normal of the surface. To achieve this, we will use the dot product. Add the following below the line declaring <code>edgeDepth</code>.</p>			
<pre><code class="language-glsl">float3 viewNormal = normal0 * 2 - 1;
float NdotV = 1 - dot(viewNormal, -i.viewSpaceDir);

return NdotV;</code></pre>
			<p>When the view normal is sampled from <code>_CameraNormalsTexture</code> it is the range 0...1, while <code>i.viewSpaceDir</code> is in the -1...1 range. We transform the view normal so that both normals are in the same range, and then take the dot product between the two.</p>
			<div class="figure">
				<img src="/media/tutorials/outline-rim.png"></img>
			</div>
			<p>As the angle between the normal and the camera increases, the result of the dot product gets larger (as we have inverted it). We want <code>depthThreshold</code> to get larger as the angle increases, too. We could just multiply it by <code>NdotV</code>, but we'll manipulate the value a bit beforehand to gain more control. We will construct a variable called <code>normalThreshold</code>, and multiply <code>depthThreshold</code> by it.</p>
			<p>Currently, <code>NdotV</code> ranges from -1...1. We are going to first rescale the value to the 0...1 range to make it easier to work with. We will add a lower bound cutoff, since it is unnecessary to modify the threshold of surfaces that are mostly facing the camera.</p>
<pre><code class="language-glsl">// Add below the line declaring NdotV.
float normalThreshold01 = saturate((NdotV - _DepthNormalThreshold) / (1 - _DepthNormalThreshold));</code></pre>
			<p>The above equation takes all values of <code>NdotV</code> in the range from <code>_DepthNormalThreshold</code> to 1, and rescales them to be 0...1. By having a lower bound in this way, we are able to apply our new threshold <em>only</em> when surfaces are above a certain angle from the camera. This equation is exposed in Unity as <a href="https://docs.unity3d.com/ScriptReference/Mathf.InverseLerp.html" target="_blank">Mathf.InverseLerp</a>, where <strong>a</strong> is <code>_DepthNormalThreshold</code>, <strong>b</strong> is 1, and <strong>value</strong> is <code>NdotV</code>.</p>
			<p>Before we multiply it into <code>depthThreshold</code>, we want to do one final transformation of the range. We will take it from 0...1 to instead be from 1 to an upper bound we will define as <code>_DepthNormalThresholdScale</code>.</p>
<pre><code class="language-glsl">// Add below the line declaring normalThreshold01.
float normalThreshold = normalThreshold01 * _DepthNormalThresholdScale + 1;</code></pre>
			<p>With that done, we can multiply in our value and expose our new variables to the inspector.</p>
<pre><code class="language-glsl">// Modify the existing line declaring depthThreshold.
float depthThreshold = _DepthThreshold * depth0 </code><span class="modified"><code>* normalThreshold</code></span><code class="language-glsl">;

…

// Add as new variables.
float _DepthNormalThreshold;
float _DepthNormalThresholdScale;

…

// Remove the debug return call.
</code><span class="removed"><code>return NdotV;</code></span></pre>
<pre><code class="language-csharp">// Add to PostProcessOutlineRenderer.
[Range(0, 1)]
public FloatParameter depthNormalThreshold = new FloatParameter { value = 0.5f };
public FloatParameter depthNormalThresholdScale = new FloatParameter { value = 7 };

…

// Add to PostProcessOutlineRenderer.
sheet.properties.SetFloat("_DepthNormalThreshold", settings.depthNormalThreshold);
sheet.properties.SetFloat("_DepthNormalThresholdScale", settings.depthNormalThresholdScale);</code></pre>
			<div class="figure">
				<img src="/media/tutorials/outline-artifact-removed.png"></img>
			</div>
			<h2>4. Final composition</h2>
			<p>With our outlines looking nice and clean, we can now blend them together with the scene. Before doing so, we'll add a new property to control the color of the outlines.</p>
<pre><code class="language-glsl">// Add just above the line declaring float4 color.
float4 edgeColor = float4(_Color.rgb, _Color.a * edge);

…

// Add as a new variable.
float4 _Color;</code></pre>
<pre><code class="language-csharp">// Add to PostProcessOutlineRenderer.
public ColorParameter color = new ColorParameter { value = Color.white };

…

// Add to PostProcessOutlineRenderer.
sheet.properties.SetColor("_Color", settings.color);</code></pre>
			<p>Lastly, we'll blend the color sampled from the scene with the outlines, using the <code>alphaBlend</code> function.</p>
<pre><code class="language-glsl">// Remove the debug return call.
</code><span class="removed"><code>return edge;</code></span><code class="language-glsl">

…

// Replace the existing return color line, at the very end of the fragment shader.
</code><span class="modified"><code>return alphaBlend(edgeColor, color);</code></span></pre>
			<div class="figure">
				<img src="/media/tutorials/outline-final.png"></img>
				<div class="figure-info">Final blended composition, with <strong>Subpixel Morphological Anti-aliasing</strong> set to <strong>High</strong>.</div>
			</div>
			<h2>Conclusion</h2>
			<p>This shader can be used to generate a wide variety of effects and styles. Disabling anti-aliasing and pairing with a dither overlay can output a retro look similar to <a href="https://store.steampowered.com/app/653530/Return_of_the_Obra_Dinn/" target="_blank">Return of the Obra Dinn</a>. Adding variable edge thickness via dilation can create strokes reminiscent of 2D illustration, as described in <a href="https://graphics.pixar.com/library/ToonRendering/paper.pdf" target="_blank">this paper by Pixar</a>.</p>
			<a class="button" href="https://github.com/IronWarrior/UnityOutlineShader" target="_blank">
				<span>View source</span>
				<span class="button-type">GitHub repository</span>
			</a>